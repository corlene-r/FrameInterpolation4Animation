## Master List of Files I Altered:

Some may be missing; trying to keep a master list for my own reference

Edited: 

1. `inference_img.py`, `inference_video.py`: Minor edits to get working on mac
2. `dataset.py`: Getting working with proper dataset
3. `model/warpplayer.py`: changed the padding mode to border
4. `RIFE.py`, `IFNet.py` as well as derivatives of this name: Getting it working on mac; the downloaded train log needs to be edited
5. `train.py`: Edits to get working on mac, take new parameters
6. `model/laplacian.py`: Edits to get working on mac

Created: 

1. `run_training.sh`, `get_all_model_outputs.sh`: Shell scripts to automate training
2. `preprocess_animeinterp_data.py`: Data to convert animeinterp data to usable format
3. `model/crossentropy.py`: Getting a new loss function created to work
4. `final_models` and all contents in folder generated by my training
5. `test_imgs`, `output` and all folder contents: used for testing, generating results 

Additionally, I ran a lot of training with params. Note that this ignores setup work for conda environment, which is detailed later in this doc. Some intermittent shell scripts for accomplishing specific tasks have been deleted. 


## Action Plan (for self-organization)

1. Get it working again like i had last semester - **CHECK**
2. Be able to run DAIN fine-tuning scripts on their data on my computer - **CHECK**
3. Run DAIN fine-tuning scripts on my data on my computer - **CHECK**
4. Fine tune the loss function to what I proposed - **CHECK**
5. Fiddle with results and toy around - **CHECK**


## More Detailed Log of Work & How I Ran
### Setting up environment and changes to `requirements.txt`

Make a conda environment with python 3.8.5 and activate it

After running `pip install -r requirements.txt`, there's some issues with using the mac to be compatible. That's why you should follow up the following two commands, which will get a version of torch compatible with Mac OS 13.5.1

```
pip uninstall torch torchvision 
pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu
```
   -> Note: if the nightly thing doesn't wind up working out, if you can get a past version, I
            installed the above on Aug. 19, so that date should hopefully work out fine

Next, enable the MPS fallback variable in the python environment (this makes sure that some of the stuff that would be sent to the GPU can be done on your mac). 

```
conda env config vars set PYTORCH_ENABLE_MPS_FALLBACK=1 
```

Test running the inference image script using:

```
python inference_img.py --img mytests/gf1.png mytests/gf3.png --exp=1
```

Hopefully it works! If not, guess you're screwed lol (note: I edited warpplayer.py tp remove padding_mode='border', this might've messed something up)


### Running train script on their data

usage: train.py [-h] [--epoch EPOCH] [--batch_size BATCH_SIZE] [--local_rank LOCAL_RANK] [--world_size WORLD_SIZE] [--mps MPS]

Needed to install 2 new modules (so far):

```
pip install tensorboard
pip install packaging
```

Also, torchvision needs to be reinstalled properly

```
pip uninstall torch torchvision
pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu
```

To run initially, I used `python train.py --mps=True --epoch=10`. You should probably make sure that mps is true if running on a mac; run `python train.py -h` for list of args (or read the train.py code)

### Prepping data

I used data from https://github.com/lisiyao21/AnimeInterp , specifically downloaded from https://drive.google.com/file/d/1XBDuiEgdd6c0S4OXLF4QvgSn_XNPwc-g/view . The github describes their dataset in further detail. 

All the data from AnimeInterp is in the wrong format for the RIFE model initially. Once you have downloaded those files, run `preprocess_animeinterp_data.py`. This is required to get everything in the correct format (we need pngs, animeinterp gives jpgs). Additionally, a master list of files is used to read the data, and this sets that up properly. 

As there are a lot of images, this takes a while to run. I only ran this a single time, then used the reformatted data for all subsequent things. 

### Running training script on my data

I generally use the shell script `run_training.sh` to run training. My computer gets angry when running more than 20 epochs or so, so I run in batches of 15, saving the intermittent models with epoch number in the filename. 

I'll fiddle with it and set epochs appropriately; for example, I choose to run epochs 255-300 for a few of the models I tested. To do this, I'll just change the bounds of the sequence. 

After I implemented my own loss function, you can specify the parameter `--lossfun` as `laploss` (default; their loss function) or `crossentropy` (my loss function)

### Implementing my loss function

I did my loss function as additive between a cross entropy loss function and the gaussian loss. This uses the file `model/crossentropy.py`. This adds a cross entropy loss based on pixel values (with 10 values for each R, G, and B pixel) to the l2 loss as found in `model/laplacian.py`. The two losses are added, and the parameter shows how much the cross entropy loss contributes (higher number -> higher contribution)

### Continuing to run training

Run training by changing the weight in the `model/crossentropy.py` file (ln 66, col 16). I ran model for the 3 parameter from epochs 0-300; however, all other models I ran 255-300. This just saves like 2 days of runtime, so I think it's worth the tradeoff. 

### Reformatting for submission

All final models are in the `final_models` folder, and reflect epoch 300 for each model as specified. `l2_loss` is their loss function, `param=##` is my loss function with `##` being the value of weighting parameter in the `model/crossentropy.py` file. I haven't included all saved models because I didn't think they were relevant for submission.